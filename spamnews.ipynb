{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMmGUkiIG2D1l56DrRHYkWg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pavannaik321/spam-news/blob/main/spamnews.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##importing libraries\n"
      ],
      "metadata": {
        "id": "X6m1jRnaWIBG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0mqoyRREVtU7"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "True_news = pd.read_csv('True.csv')\n",
        "Fake_news = pd.read_csv('Fake.csv')\n"
      ],
      "metadata": {
        "id": "TSQ_GzbbWVQe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "True_news"
      ],
      "metadata": {
        "id": "dqUzCHDZWqcO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Fake_news"
      ],
      "metadata": {
        "id": "smacbhkMWtLo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "adding lable"
      ],
      "metadata": {
        "id": "TtJ05h5GW1zU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "True_news['label'] = 0\n",
        "Fake_news['label'] = 1\n"
      ],
      "metadata": {
        "id": "AKXRfOW4WzYo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset1 = True_news[['text','label']]\n",
        "dataset2 = Fake_news[['text','label']]"
      ],
      "metadata": {
        "id": "XzriTkTYXE0X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = pd.concat([dataset1,dataset2])\n",
        "dataset\n",
        "dataset.shape"
      ],
      "metadata": {
        "id": "8h4eVEvSXUPi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check for null valuesf\n",
        "dataset.isnull().sum()"
      ],
      "metadata": {
        "id": "_Avteg8-XfXz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#chaek for correct news and fake news\n",
        "dataset['label'].value_counts()"
      ],
      "metadata": {
        "id": "ishYBMYeX12P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#shuffle the datasets becaues the labels are in 00000 111111\n",
        "dataset = dataset.sample(frac=1)\n",
        "dataset"
      ],
      "metadata": {
        "id": "3vK92BZiYQDp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#encode and clean by NLP"
      ],
      "metadata": {
        "id": "2Q7dwD2hYg8c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import re #rejex used for cleaning of data\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer"
      ],
      "metadata": {
        "id": "W1nwhO9IYrkB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ps = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "ut0rH-5tZKn-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords = stopwords.words('english')"
      ],
      "metadata": {
        "id": "yc2os6IyZOM0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "id": "mbFOdCKe4yaO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#function to clean a row\n",
        "def clean_row(row):\n",
        "  row = row.lower() #converting to lowercase our wish\n",
        "  row = re.sub('[^a-zA-Z]',' ',row) #from text remove number and special characters\n",
        "\n",
        "  token = row.split()\n",
        "  news = [ps.lemmatize(word) for word in token if not word in stopwords] # a an the are some of the stopwords we have to remove all those words\n",
        "\n",
        "  cleanned_news = ' '.join(news)\n",
        "\n",
        "  #example\n",
        "  #my name is pavan and my field is AI\n",
        "\n",
        "  #my name is pavan and field AI\n",
        "  #0    1    2   3    4    5   6 it will add vector\n",
        "  #(my is) is removed\n",
        "  return cleanned_news"
      ],
      "metadata": {
        "id": "FWUEuX8G44gd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset['text']"
      ],
      "metadata": {
        "id": "iy9xzTn26Vfw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset['text'] = dataset['text'].apply(lambda x : clean_row(x))"
      ],
      "metadata": {
        "id": "aUsUxbEU6qoi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset['text']\n"
      ],
      "metadata": {
        "id": "ZzKz2BiB646M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#the text will be converted to vector\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "metadata": {
        "id": "etj9YPic7VG7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create instance\n",
        "vectorizer = TfidfVectorizer(max_features= 50000 , lowercase = False , ngram_range=(1,2))"
      ],
      "metadata": {
        "id": "7UuyopgQ7eYr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#now testing and training\n",
        "X = dataset.iloc[:35000,0] # first column\n",
        "Y = dataset.iloc[:35000,1] # second column"
      ],
      "metadata": {
        "id": "12QXjvFo7uPG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#spiliting the dataset\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "1hgKBwFj8xH8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data, test_data , train_label , test_label = train_test_split(X , Y , test_size = 0.2 , random_state = 0)"
      ],
      "metadata": {
        "id": "-NHDL0wD87sD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#lets vectorize the data\n",
        "#convert all the text to numbers the range will be till 50000\n",
        "vec_train_data = vectorizer.fit_transform(train_data)\n",
        "vec_test_data = vectorizer.fit_transform(test_data)\n",
        "\n",
        "#convert the trainig and testing data to array\n",
        "vec_train_data = vec_train_data.toarray()\n",
        "vec_test_data = vec_test_data.toarray()\n"
      ],
      "metadata": {
        "id": "00crM_ub9W5Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#checking the shape\n",
        "vec_train_data.shape , vec_test_data.shape\n",
        "\n",
        "#example\n",
        " # i am pavan\n",
        " # i      1 0 0\n",
        " # am     0 1 0\n",
        " # pavan  0 0 1\n"
      ],
      "metadata": {
        "id": "anOXSbi-9WPC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating trainig data\n",
        "training_data = pd.DataFrame(vec_train_data , columns = vectorizer.get_feature_names())\n",
        "testing_data = pd.DataFrame(vec_test_data, columns = vectorizer.get_feature_names())\n",
        "\n",
        "training_data"
      ],
      "metadata": {
        "id": "P3dAzWaV_AO4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Model Naive bias  (training data)"
      ],
      "metadata": {
        "id": "wy7LIJfR_xMz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import MultinomialNB"
      ],
      "metadata": {
        "id": "yeVybF4yABlJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clf = MultinomialNB()\n"
      ],
      "metadata": {
        "id": "QTAK1nLkALbl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clf.fit(train_data , train_label)\n"
      ],
      "metadata": {
        "id": "U256KFcOASJS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y_pred = clf.predict(testing_data)\n",
        "test_label"
      ],
      "metadata": {
        "id": "EewUFLd2AW8I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y_pred"
      ],
      "metadata": {
        "id": "1jvYXvnIBqTd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check accuracy\n",
        "from sklearn.metrics import accuracy_score\n"
      ],
      "metadata": {
        "id": "SSYbtf_3BtzB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_score(test_label , Y_pred)"
      ],
      "metadata": {
        "id": "U1xCxNUtB4es"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}